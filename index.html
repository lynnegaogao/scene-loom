<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="SceneLoom: Communicating Data with Scene Contex.">
  <meta name="keywords" content="Paper, website">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SceneLoom: Communicating Data with Scene Context</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>




  <section class="hero">
    <div class="body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <span class="publication-title">
              <span style="display: inline-flex; align-items: center; gap: 8px;">
                <img src="./static/images/logo.png" alt="SceneLoom Logo" style="height: 1.9em; vertical-align: middle;">
                Communicating Data with Scene Context
              </span>
            </span>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://lynnegao.me/">Lin Gao</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://shenleixian.github.io/">Leixian Shen</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://zyh1222.github.io/">Yuheng Zhao</a><sup>1</sup>,
                <span class="author-block">
                  <a>Jiexiang Lan</a><sup>1</sup>,
                  <span class="author-block">
                    <a href="http://huamin.org/">Huamin Qu</a><sup>2</sup>,
                    <span class="author-block">
                      <a href="http://simingchen.me/">Siming Chen</a><sup>1</sup>,
            </div>

            <div class="publication-authors">
              <span class="author-affiliation-block"><sup>1</sup>Fudan University, Shanghai, China</span><br>
              <span class="author-affiliation-block"><sup>2</sup>The Hong Kong University of Science and Technology,
                Hong Kong SAR, China </span>
            </div>

            <!--<div class="column has-text-centered">-->
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://lynnegao.me/papers/2025-SceneLoom-VIS.pdf">
                  <span>
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.16466">
                  <span>
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <span class="link-block">
                <a href="./static/files/Appendix.pdf" target="_blank">
                  <span><i class="fas fa-file-alt"></i></span>
                  <span>Appendix</span>
                </a>
              </span>

              <span class="link-block">
                <a href="./static/files/Poster.pdf" target="_blank">
                  <span><i class="fas fa-scroll"></i></span>
                  <span>Poster</span>
                </a>
              </span>

              <!-- Video Link. -->
              <!--<span class="link-block">
                <a href="https://drive.google.com/file/d/19eXsSnTAeD-RtIiDqfpjh1bb6WyHYd8R/view">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>-->
              <!-- Code Link. -->
              <!--<span class="link-block">
                <a href="https://github.com/TUM-AVS/paper-website-project"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>-->
              <!-- Dataset Link. -->
              <!--<span class="link-block">
                <a href="https://github.com/TUM-AVS/paper-website-project"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>-->
              <!--</div>-->

              <!--</div>-->
            </div>
          </div>
        </div>
      </div>
  </section>

  <section style="padding: 20px 5px;">
    <div class="container is-max-desktop is-four-fifths">
      <!--<div class="body">-->
      <!--<img src="./static/images/teaser.jpg"
                 class="teaser-image"
                />-->
      <!-- You can also add a video as a teaser -->

      <video autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4" type="video/mp4">
      </video>

      <!--</div>-->
    </div>
  </section>



  <section>
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <span class="section-title">Introduction</span>
          <div class="section-content">
            <p>
              In data-driven storytelling contexts such as data journalism and data videos, data visualizations are
              often presented alongside real-world imagery to support narrative context.
              However, these visualizations and contextual images typically remain separated, limiting their combined
              narrative expressiveness and engagement.
              Achieving this is challenging due to the need for fine-grained alignment and creative ideation.
              To address this, we present SceneLoom, a Vision-Language Model (VLM)-powered system that facilitates the
              coordination of data visualization with real-world imagery based on narrative intents.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      <!-- Paper video. -->

      <div class="columns is-centered">
        <div class="column is-four-fifths" style="margin-bottom: 20px;">
          <!--<span class="section-title">Introduction Video</span>-->
          <div class="video-container" style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
            <iframe src="https://www.youtube.com/embed/I_qUDy52uXk" title="SceneLoom Introduction Video" frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
            </iframe>
          </div>
        </div>
      </div>


    </div>
    </div>
    <!--/ Paper video. -->
    </div>
  </section>


  <section>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <span class="section-title">Methods</span><br>
          <img src="./static/images/methods.png" style="width:80%; display: block; margin: 0 auto;"/>
          <div class="section-content">
            <p>
              Data visualizations and real-world scenes differ fundamentally in information type, perception modes, and
              communicative goals.
              This divergence creates tensions in their coordination:
            <ul class="custom-bullets">
              <li><span class="highlight">Semantic gaps</span> between abstract data encoding and concrete scene
                semantics.</li>
              <li><span class="highlight">Perceptual competition</span> when visual channels overlap.</li>
            </ul>
            </p>
          </div>

          <span class="section-subtitle">Formative Study</span>
          <div class="section-content">
            <p style="margin-bottom: 10px;">
              To address these issues, we conducted a formative study to analyze design components in data
              visualizations and real-world scenes,
              and to derive coordination relationships from both visual and semantic perspectives.
            </p>
            <p style="margin-bottom: 10px;">
              We collected 54 data videos that integrate visualizations with real-world imagery, identifying recurring
              patterns and common coordination strategies.
              The full set of cases and coding results is available in the following online table.
            </p>
            <iframe class="airtable-embed" src="https://airtable.com/embed/apparxcuOrUlTeKj3/shrFcnYr0QytfWLeE"
              frameborder="0" onmousewheel="" width="100%" height="533"
              style="background: transparent; border: 1px solid #ccc;"></iframe>
            <p style="margin-top: 10px;">
              Based on corpus analysis, we identified key visual and semantic components from both data visualizations
              and real-world scenes and organized them into a design space structured along two main dimensions:
            <ul class="custom-bullets">
              <li><span class="highlight">Visual alignment</span>, which ensures spatial and perceptual consistency.
              </li>
              <li><span class="highlight">Semantic coherence</span>, which maintains meaningful links between data
                content and scene context.</li>
            </ul>
            </p>
          </div>

          <span class="section-subtitle">Prototype System</span>
          <div class="section-content">
            <p style="margin-bottom: 10px;">
              Building on prior insights, we developed SceneLoom, a prototype system that implements coordination
              strategies through a structured workflow:
            <ul class="custom-bullets">
              <li><span class="highlight">Data preparation</span>: SceneLoom takes narrative text, structured data, and
                real-world images as input, extracts narrative features, generates candidate visualizations, and filters
                segmented image elements for design coordination.</li>
              <li><span class="highlight">Visual perception</span>: A specification format is introduced to encode
                visual and semantic properties of both data visualizations and image elements, enabling consistent
                interpretation by VLMs.</li>
              <li><span class="highlight">Reasoning and mapping</span>: The system aligns visual components using
                spatial and semantic cues, supports both data-level and view-level adjustments, invokes tools through
                structured prompts, and evaluates results based on accuracy, clarity, and salience.</li>
            </ul>

            </p>

          </div>
        </div>
      </div>
  </section>

  <section>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <span class="section-title">Example Gallery</span>
          <div class="section-content">
            <p>
              Here are some representative examples of SceneLoom's generated designs.
            </p>
          </div>
          <div class="columns is-multiline">
            <!-- 第1行 -->
            <div class="column is-half">
              <video controls width="100%">
                <source src="./static/videos/video1.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <div class="column is-half">
              <video controls width="100%">
                <source src="./static/videos/video2.mp4" type="video/mp4">
              </video>
            </div>

            <!-- 第2行 -->
            <div class="column is-half">
              <video controls width="100%">
                <source src="./static/videos/video3.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-half">
              <video controls width="100%">
                <source src="./static/videos/video4.mp4" type="video/mp4">
              </video>
            </div>

            <!-- 第3行 -->
            <div class="column is-half">
              <video controls width="100%">
                <source src="./static/videos/video5.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-half">
              <video controls width="100%">
                <source src="./static/videos/video6.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>

      </div>
    </div>
  </section>





  <section class="section">
    <div class="container is-max-desktop">
      <span class="bibtex-title">BibTeX</span>
      <pre><code>@misc{gao2025sceneloomcommunicatingdatascene,
      title={SceneLoom: Communicating Data with Scene Context}, 
      author={Lin Gao and Leixian Shen and Yuheng Zhao and Jiexiang Lan and Huamin Qu and Siming Chen},
      year={2025},
      eprint={2507.16466},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2507.16466}, 
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="columns is-centered">
      <div class="footer-content">
        <center>
          <p>
            This website (<a href="https://github.com/paper-website/paper-website.github.io">source code</a>) was
            adapted from the popular <a href="https://nerfies.github.io">Nerfies</a> project page and is licensed
            under a <br><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </center>
      </div>
    </div>
  </footer>

</body>

</html>